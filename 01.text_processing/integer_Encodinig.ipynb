{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integer Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \" A barber is a person. a barber is good person. a barber is huge person.\\\n",
    "            he knew A Secret! The Secret He Kept is huge secret. Huge Secret. His barber kept his word.\\\n",
    "            a barber kept his word.His barber kept his secret.\\\n",
    "            But keeping and keeping such a huge secret to himself was driving the barber crazy.\\\n",
    "            the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dicionary를 사용하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화:  [' A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge Secret.', 'His barber kept his word.', 'a barber kept his word.His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화            \n",
    "sentences = sent_tokenize(raw_text)\n",
    "print(\"문장 토큰화: \" ,sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정제 작업과 정규화 작업을 수행하여 단어 토큰화를 진행   \n",
    "단어들을 소문자화해서 단어의 개수를 통일 시키고, 불용어와 단어 길이가 2 이하인 경우 단어를 일부 제외시킴\n",
    "텍스트를 수치화하는 단계라는 것은 본격적으로 자연어 처리 작업을 한다는 뜻이므로, 단어가 텍스트 일때만 할 수 있는 최대한 전처리를 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추가로 단어 토큰화를 수행한 경우:  [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word.his', 'barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
      "단어 집합:  {'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 1, 'word.his': 1, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "preprocessed_sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for sentence in sentences:\n",
    "    # 단어 토큰화 수행\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    result = []\n",
    "    \n",
    "    for word in tokenized_sentence:\n",
    "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄여준다.\n",
    "        if word not in stop_words:  # 단어 token화 된 결과에 대해서 불용어 제거\n",
    "            if len(word)>2: # 단어 길이가 2 이하라면 제거\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0\n",
    "                vocab[word] += 1\n",
    "    preprocessed_sentences.append(result)\n",
    "print(\"추가로 단어 토큰화를 수행한 경우: \", preprocessed_sentences)\n",
    "print('단어 집합: ', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python의 dictionary 구조로 단어를 key, 단어의 빈도수를 value로 저장해놓았다.\n",
    "vocab에 단어를 입력하면 빈도수를 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barber가 나온 횟수는??  8\n"
     ]
    }
   ],
   "source": [
    "print('barber가 나온 횟수는?? ', vocab[\"barber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('keeping', 2), ('good', 1), ('knew', 1), ('word', 1), ('word.his', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 빈도수가 높은 순서대로 정렬\n",
    "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse=True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "높은 빈도수를 가진 단어일수록 낮은 정수를 부여.\\\n",
    "정수는 1부터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'keeping': 6}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab_sorted:\n",
    "    if frequency >1: # 빈도수가 낮은 단어는 제외\n",
    "        i = i+1\n",
    "        word_to_index[word] = i\n",
    "        \n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1의 index를 가진 단어가 가장 빈도수가 높은 단어!\\\n",
    "이러한 작업을 수행하는 동시에 각 단어의 빈도수를 알 경우에만 할 수 있는 전처리인 빈도수가 적으 단어를 제외시키는 작업을 수행했다.\\\n",
    "등장 빈도가 낮은 단어는 자연어 처리에서 의미를 가지지 않을 가능성이 높기 때문이다\\\n",
    "여기에서는 빈도수가 1인 단어들은 전부 제외시켰다\n",
    "\n",
    "자연어 처리를 하게 되면, text data에 있는 단어들을 모두 사용하기 보다는 빈도수가 가장 높은 n개의 단어만 사용하는 경우가 많다\\\n",
    "위 단어들은 빈도수가 높은 순으로 낮은 정수가 부여되어있으므로 빈도수 상위 n개의 단어만 사용하고 싶다면, vocab에서 정수의 값이 1부터 n까지인\\\n",
    "단어들만 사용하면 된다. 상위 5개의 단어만 사용한다고 가정해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "\n",
    "# index가 5 초과인 단어를 제거(빈도수가 낮은 단어를 제거함)\n",
    "words_frequency = [word for word, index in word_to_index.items() if index >= vocab_size + 1]\n",
    "\n",
    "# 해당 단어에 대한 index 정보를 삭제\n",
    "for w in words_frequency:\n",
    "    del word_to_index[w]\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_to_index에는 빈도수가 높은 상위 5개의 단어만 저장되었다.\\\n",
    "word_to_index를 사용하여 단어 token화가 된 상태로 저장된 sentences에 있는 각 단어를 정수로 바꾸는 작업을 진행\n",
    "\n",
    "예를 들어 sentences에서 첫번째 문장은 ['barber','person']이 있는데,이 문장에 대햇서는 [1, 5]로 인코딩한다.\\\n",
    "하지만 두번째 문장인 ['barber','good','person']에는 더 이상 word_to_index에는 존재하지 않는 단어인 'good'이 존재한다. (good은 빈도수가 낮아 삭제됨)\n",
    "\n",
    "이처럼 단어 집합에 존재하지 않는 단어들이 생기는 상황을 out of Vocabulary(단어 집합에 없는 단어) 문제라고 한다. ('약자로 OOV 문제')\\\n",
    "word_to_index에 OOV란 단어를 새롭게 추가하고,단어 집합에 없는 단어들은 'OOV'의 index로 encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n"
     ]
    }
   ],
   "source": [
    "word_to_index['OOV'] = len(word_to_index) + 1\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_to_index를 이용하여 sentences의 모든 단어들을 mapping되는 정수로 만들 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6, 1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences = []\n",
    "for sentence in preprocessed_sentences:\n",
    "    encoded_sentence = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            # 단어 집합에 있는 단어라면 해당 단어의 정수를 return\n",
    "            encoded_sentence.append(word_to_index[word])\n",
    "        except KeyError:\n",
    "            # 만약 단어 집합에 없는 단어라면 'OOV' 정수를 return\n",
    "            encoded_sentence.append(word_to_index['OOV'])\n",
    "    encoded_sentences.append(encoded_sentence)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## counter 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word.his', 'barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word.his', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "# words = np.hstack(preprocessed_sentences)으로도 수행이 가능하다\n",
    "all_words_list = sum(preprocessed_sentences, [])\n",
    "print(all_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 python의 counter 입력으로 사용하면 중복을 제거하고 단어의 빈도수를 기록한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'keeping': 2, 'good': 1, 'knew': 1, 'word': 1, 'word.his': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "# python의 counter 모듈을 이용하여 단어의 빈돗수를 count\n",
    "\n",
    "vocab = Counter(all_words_list)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어를 key로, 단어에 대한 빈도수가 value로 저장되어 있다.\\\n",
    "vocab에 단어를 입력하면 빈도수를 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# 'barber'라는 단어의 빈도수를 출력\n",
    "print(vocab[\"barber\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "barber란 단어가 총 8번 등장했다.\\\n",
    "most_common()는 상위 빈도수를 가진 주어진 수의 단어만 return.\\\n",
    "이를 이용하여 등장 빈도수가 높은 단어들을 원하는 개수만큼 얻을 수 있다.\\\n",
    "등장 빈도수 상위 5개의 단어만 단어 집합으로 저장해볼 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size)   # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "높은 빈도수를 가진 단어일 수록 정수 index가 낮음을 잊지 말 것!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab:\n",
    "    i = i+1\n",
    "    word_to_index[word] = i\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK의 FreqDist 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK에서는 빈도수 계산 도구인 FreqDist()를 지원한다.\\\n",
    "위에서 사용한 Counter와 같은 방법으로 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.hstack으로 문장 구분을 제거\n",
    "vocab = FreqDist(np.hstack(preprocessed_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어를 key로, 단어에 대한 빈도수가 value로 저장되어 있다\\\n",
    "vocab에 단어를 입력하면 빈도수를 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"barber\"])  # \"barber\"의 빈도수를 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most_common => 상위 빈도수를 가진 주어진 수의 단어만 return\\\n",
    "\n",
    "이를 이용하여 등장 빈도수가 높은 단어들을 원하는 개수만큼 얻을 수 있다\\\n",
    "등장 빈도수가 상위 5개인 단어만 단어 집합으로 저장해볼 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size)   # 등장 빈도수가 높은 상위 5개만 출력\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞의 Counter()를 사용했을 때와 동일한 결과\\\n",
    "여전히 높은 빈도수를 가진 단어일수록 낮은 정수 index를 부여\\\n",
    "짧게 하고 싶다면 enumerate()를 이용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {word[0]: index  + 1 for index, word in enumerate(vocab)}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## enumerate\n",
    "\n",
    "enumerate()는 순서가 있는 자료형(list,set,tuple,dictionary,string)을 입력으로 받아 index를 순차적으로 함께 return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value : a, index: 0\n",
      "value : b, index: 1\n",
      "value : c, index: 2\n",
      "value : d, index: 3\n",
      "value : e, index: 4\n"
     ]
    }
   ],
   "source": [
    "test_input = ['a', 'b', 'c','d','e']\n",
    "for index, value in enumerate(test_input):  # 입력의 순서대로 0부터 index 부여\n",
    "    \n",
    "    print(\"value : {}, index: {}\". format(value, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list의 모든 token에 대해서 index가 순차적으로 증가되며 부여된 것을 확인 할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
