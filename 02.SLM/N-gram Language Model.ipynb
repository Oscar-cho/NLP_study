{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram 언어 모델\n",
    "\n",
    "n-gram 언어 모델은 count에 기반한 통계적 접근을 사용한 SLM의 일종\\\n",
    "\\\n",
    "등장한 모든 단어를 고려하는 것이 아니라 일부 단어만 고려하는 접근 방법을 사용.\\\n",
    "n: 일부 단어를 몇개를 가질 것인지 정하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Corpus에서 count하지 못하는 경우 감소\n",
    "\n",
    "SLM(statistical Language Model)의 한계는 train corus의 확률을 계산하고 싶은 문장이나 단어가 없을 수 있다\\\n",
    "확률을 계산하고 싶은 문장이 길어질수록 갖고 있는 corpus에서 그 문장이 존재하지 않을 가능성이 높다\\\n",
    "즉, count할 수 없을 가능성이 높다.\n",
    "\n",
    "이때, **참고하는 단어들을 줄이면 count 할 수 있는 가능성을 높일 수 있다**. \n",
    "\n",
    "𝑃 (is|An adorable little boy) ≈ 𝑃 (is|boy)\n",
    "\n",
    "An adorable little boy가 나왔을 때, is가 나올 확률을 그냥 boy가 나왔을 때, is가 나올 확률로 생각해보자\\\n",
    "갖고 있는 corpus에 An adorable little boy is가 있을 가능성보다는 boy is라는 더 짧은 단어 sequence가 존재할 가능성이 더 높다\n",
    "\\\n",
    "지나친 일반화 같다면 little boy가 나왔을 때 is가 나올 확률로 생각해보자\n",
    "\n",
    "𝑃 (is|An adorable little boy) ≈ 𝑃 (is|little boy)\n",
    "\n",
    "기존의 An adorable little boy가 나왔을 때 is가 나올 확률을 구하기 위해서는 An adorable little boy가 나온 획수와 An adorable boy is가 나온 횟수를\\\n",
    "모두 count  해야했으나, 단어의 확률을 구하고자 기준 단어의 앞 단어를 전부 포함하여 count 하는게 아니라, 앞 단어 중 **임의의 개수(n개)** 만 포함하여 count하여 근사하자는 개념\n",
    "\\\n",
    "이 과정을 거쳐 corpus에서 해당 단어의 sequence를 count할 확률이 높아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. N-gram\n",
    "\n",
    "![\n",
    "    \n",
    "](attachment:image-2.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
