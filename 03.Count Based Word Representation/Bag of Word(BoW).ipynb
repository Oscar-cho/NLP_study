{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words(BoW)\n",
    "\n",
    "Bag of Words: 단어의 등장 순서를 고려하지 않고, 빈도수 기반의 단어 표현 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag of Words\n",
    "\n",
    "Bag of Words란 단어의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도에만 집중하는 택스트 데이터의 수치화 표현 방법\\\n",
    "**Bag of Words** -> 단어들의 가방\\\n",
    "어떤 text 문서에 있는 단어들을 가방에다가 전부 집어 넣고, 가방을 흔들어 단어들을 섞는다.\\\n",
    "만약, 해당 문서 내에 특정 단어가 N번 등장 -> 가방에는 특정 단어가 N개.\\\n",
    "가방을 흔들었기 때문에 단어의 순서는 중요한 것이 아님\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW를 만드는 과정을 두 가지 과정으로 생각!\n",
    "\n",
    "1) 각 단어에 고유한 정수 index를 부여   -> 단어 집합 생성\n",
    "2) 각 index의 위치에 단어 token의 등장 횟수를 기록한 vector를 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**문서 1: 정부가 발표하는 물가상승률과 소비자가 느끼는 물가 상승률은 다르다** \\\n",
    "\\\n",
    "입력된 문서에 대해 ```단어 집합(vocabulary)``` 를 만든 후 , 각 단어에 정수 index를 할당, BoW를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def build_bag_of_words(document):\n",
    "    # 온점 제거 및 형태소를 분석\n",
    "    document = document.replace('.','')\n",
    "    tokenized_document = okt.morphs(document)\n",
    "\n",
    "    word_to_index = {}\n",
    "    bow = []\n",
    "\n",
    "    for word in tokenized_document:\n",
    "        if word not in word_to_index.keys():\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "            # BoW에 전부 기본값인 1을 넣는다\n",
    "            bow.insert(len(word_to_index) - 1,1)\n",
    "        else:\n",
    "            # 재등장하는 단어의 index\n",
    "            index = word_to_index.get(word)\n",
    "            # 재등장한 단어는 해당하는 index의 위치에 1을 더함\n",
    "            bow[index] = bow[index] + 1\n",
    "\n",
    "    return word_to_index, bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: {'정북': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n",
      "bag of words vector : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"정북가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"\n",
    "vocab, bow = build_bag_of_words(doc1)\n",
    "print('vocabulary:', vocab)\n",
    "print('bag of words vector :', bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 출력 결과 -> 문서 1에 각 단어에 대해 index를 부여한 결과\\\n",
    "\\\n",
    "두 번째 출력 결과 -> 문서 1의 BoW\n",
    "\n",
    "index 4에 해당하는 물가 상승률은 두번 언급 되었기 때문에 index 4에 해당하는 값이 2이다\n",
    "\n",
    "**index는 0부터 시작하는 것에 유의** \\\n",
    "\\\n",
    "물갸 상승률의 BoW는 5번째 값.\\\n",
    "만약 한국어에서 불용어에 해당하는 조사를 제거한다면 더 정제된 BoW를 만들 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BoW의 다른 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**문서 2: 소비자는 주로 소비하는 상품을 기준으로 물가 상승률을 느낀다**\n",
    "\n",
    "위의 함수를 임의의 문서 2에 입력으로 하여 결과 확인\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  {'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가': 9, '상': 10, '승률': 11, '느낀다': 12}\n",
      "bag of words vector:  [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc2 = '소비자는 주로 소비하는 상품을 기준으로 물가 상승률을 느낀다.'\n",
    "\n",
    "vocab, bow = build_bag_of_words(doc2)\n",
    "\n",
    "print('vocabulary: ', vocab)\n",
    "print('bag of words vector: ', bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서 1과 2의 결과를 합쳐서 문서 3이라 하고, BoW를 만들어볼것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**문서3: 정부가 발표하는 물가상승률과 소비자가 느끼는 물가 상승률은 다르다. 소비자는 주로 소비하는 상품을 기준으로 물가 상승률을 느낀다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabularly:  {'정북': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '물가': 17, '상': 18, '승률': 19, '느낀다': 20}\n",
      "bag of words vector:  [1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc3 = doc1 + ' ' + doc2\n",
    "vocab, bow = build_bag_of_words(doc3)\n",
    "\n",
    "print('vocabularly: ', vocab)\n",
    "print('bag of words vector: ', bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서 3의 단어 집합은 문서 1과 문서 2의 단어들을 모두 포함하는 것을 확인할 수 있다.\\\n",
    "**BoW는 종종 여러 문서의 단어 집합을 합친 뒤에, 해당 단어 집합에 대한 각 문서의 BoW를 구할 수 있다**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW는 각 단어가 등장한 횟수로 수치화하는 text 표현 방법으로 **주로 어떤 단어가 얼마나 등장했는지를 기준으로 어떤 domain의 문서인지 판단할 때 사용** \\\n",
    "\\\n",
    "분류 문제나 여러 문제 간의 유사도를 구하는 문제에 주로 사용된다\\\n",
    "ex) '달리기', '체력', '근력' 과 같은 단어가 자주 등장 -> 체육에 관한 문서\n",
    "    '미분', '방정식', '부등식' 과 같은 단어가 자주 등장 -> 수학 관련 문서\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CountVectorizer 클래스로 BoW 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn 에서는 단어의 빈도를 count 하여 vector로 만드는 CountVectorizer 클래스를 지원\\\n",
    "\\\n",
    "이를 이용하면 영어에 대해서는 쉽게 BoW를 만들 수 있다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector :  [[1 1 2 1 2 1]]\n",
      "vocabulary :  {'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# corpus로 부터 각 단어의 빈도수를 추출\n",
    "print('bag of words vector : ', vector.fit_transform(corpus).toarray())\n",
    "\n",
    "# 각 단어의 index가 어떻게 부여되었는지 출력\n",
    "print('vocabulary : ', vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "알파벳으로 BoW를 만드는 과정에서 사라졌는데, 이는 CountVectorizer가 기본적으로 길이가 2이상인 문자에 대해서만 token으로 인식.\n",
    "\n",
    "영어는 길이가 짧은 문장을 제거하는 것 역시 전처리 작업으로 고려.\\\n",
    "\\\n",
    "**주의할점: CountVectorizer는 단지 띄어쓰기만을 기준으로 단어를 자르는 낮은 수준의 token화를 진행하고 BoW를 만든다는 것.**\\\n",
    "**-> 영어의 경우 띄어쓰기 만으로 token화가 수행되기 때문에 문제가 없으나 한국어에 CountVectorizer를 적용하면, 조사 등의 이유로 제대로 BoW가 만들어 지지 않음**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 불용어를 제거한 BoW 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불용어는 자연어 처리에서 의미를 갖지 않는 단어를 의미 \\ \n",
    "BoW를 사용한다는 것은 그 문서에서 각 단어가 얼마나 자주 등장했는지를 보여준다\\\n",
    "각 단어에 대한 빈도수를 수치화 하겠다는 것은 text 내에서 어떤 단어들이 중요한지를 보고 싶다는 의미를 함축함\\\n",
    "\\\n",
    "BoW를 만들 때, 불용어를 제거하는 일은 자연어 처리의 정확도를 높이기 위해 선택 할 수 있는 전처리 기법\\\n",
    "\\\n",
    "영어에서 Bag of Word를 만들기 위해 사용하는 CountVectorizer는 불용어를 지정하면, 불용어는 제외하고 BoW를 만들 수 있도록, 불용어 제거 기능을 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 사용자가 직접 정의한 불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector :  [[1 1 1 1 1]]\n",
      "vocabulary : {'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\",\"is\", \"not\"])\n",
    "\n",
    "print('bag of words vector : ', vect.fit_transform(text).toarray())\n",
    "print('vocabulary :', vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) CountVectorizer에서 제공하는 자체 불용어를 사용한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector :  [[1 1 1]]\n",
      "vocabulary:  {'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "print('bag of words vector : ', vect.fit_transform(text).toarray())\n",
    "print('vocabulary: ', vect.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) NLTK에서 지원하는 불용어를 사용하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector:  [[1 1 1 1]]\n",
      "vocabulary:  {'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "vect = CountVectorizer(stop_words = stop_words)\n",
    "\n",
    "print('bag of words vector: ', vect.fit_transform(text).toarray())\n",
    "print('vocabulary: ', vect.vocabulary_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
